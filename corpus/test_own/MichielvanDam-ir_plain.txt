Survey: 
A 
comparison 
of 
markov 
models 
and 
n-grams 
for 
spam 
ltering 
techniques 


Michiel 
van 
Dam 


Information 
Retrieval, 
Delft 
University 
of 
Technology, 
Mekelweg 
4, 
2628 
CD 
Delft, 
Netherlands 


m.c.vandam@student.tudelft.nl 
http://ewi.home.tudelft.nl/en/ 


Abstract. 
This 
survey 
paper 
gives 
an 
overview 
of 
spam 
ltering 
techniques 
using 
either 
markov 
models 
or 
n-grams 
A 
short 
introduction 
to 
nave 
bayesian 
classiers 
is 
given 
with 
further 
explanation 
about 
n-grams 
and 
markov 
models 
and 
a 
comparison 
is 
given 
between 
these 
techniques, 
outlining 
strengths 
and 
weaknesses. 
Based 
on 
the 
presentation 
of 
these 
approaches 
and 
the 
performance 
of 
models 
that 
were 
presented 
at 
TREC 
conferences 
2005-2007 
the 
conclusion 
can 
be 
drawn 
that 
N-gram 
based 
approaches 
have 
surpassed 
pure 
markov 
models 
especially 
for 
machine 
learning. 


Keywords: 
markov 
model, 
n-gram, 
spam, 
ltering, 
bayes 


1 
Introduction 


Spam 
is 
a 
well-known 
concept 
in 
day-to-day 
e-mail 
communication, 
and 
spam 
can 
be 
dened 
as: 
\Unsolicited, 
unwanted 
email 
that 
was 
sent 
indiscriminately, 
directly 
or 
indirectly, 
by 
a 
sender 
having 
no 
current 
relationship 
with 
the 
recipient."[
9] 


Although 
this 
is 
by 
no 
means 
a 
new 
phenomenon, 
the 
battle 
to 
limit 
spam 
is 
still 
ongoing. 
The 
countermeasures 
taken 
started 
out 
as 
manual 
intervention 
by 
administrators 
or 
a 
simple 
rule-based 
lter 
to 
block 
spam 
from 
getting 
delivered. 
With 
such 
eorts 
being 
taken, 
spam 
has 
evolved 
to 
evade 
countermeasures[6], 
upon 
which 
the 
countermeasures 
are 
evolving 
to 
defeat 
this 
attempt 
by 
spam 
to 
evade 
the 
countermeasures[7]. 
It 
is 
apparent 
this 
makes 
the 
current 
spam 
countermeasures 
complex 
and 
an 
ever-changing 
research 
eld. 


A 
spam 
lter 
can 
be 
dened 
as 
an 
automated 
technique 
to 
identify 
spam. 
For 
usual 
spam 
lters 
this 
results 
in 
a 
likelihood 
that 
a 
certain 
message 
is 
spam 
(or 
ham 
[not 
spam]). 
A 
threshold 
can 
be 
used 
to 
convert 
this 
into 
a 
binary 
spam-orham 
decision 
that 
takes 
a 
balance 
into 
account 
between 
false 
positives 
and 
false 
negatives. 
For 
nding 
a 
good 
balance 
it 
is 
important 
to 
note 
that 
classifying 
several 
spam 
messages 
as 
ham 
is 
excusable; 
people 
can 
delete 
them 
themselves, 
but 
classifying 
a 
ham 
message 
as 
spam 
is 
a 
much 
worse 
mistake, 
because 
it 
means 
the 
recipient 
will 
most 
likely 
not 
read 
the 
possibly 
important 
message. 



Michiel 
van 
Dam 


Using 
statistics 
or 
probability 
theory 
it 
is 
possible 
to 
better 
determine 
the 
likelihood 
that 
a 
certain 
message 
is 
spam. 
For 
this 
the 
Nave 
Bayes 
Classier 
is 
a 
useful 
example, 
allowing 
for 
a 
calculation 
of 
spam 
chances 
based 
upon 
multiple 
sources 
of 
evidence 
(the 
`Nave’ 
assumption 
is 
that 
all 
the 
sources 
are 
independent, 
given 
the 
class)[3] 
There 
are 
numerous 
ways 
to 
calculate 
these 
likelihoods 
or 
to 
pre-process 
a 
message 
that's 
about 
to 
be 
evaluated 
for 
spam 
content. 
Two 
of 
these 
ways 
are 
N-gram-based 
approaches 
and 
Markov 
models, 
which 
will 
both 
be 
explaned 
in 
more 
detail 
in 
subsequent 
sections. 


2 
Related 
Work 


There 
are 
lots 
of 
papers 
that 
present 
a 
survey 
of 
techniques 
of 
e-mail 
spam 
ltering 
like 
[7, 
8]. 
For 
example, 
[7] 
provides, 
next 
to 
a 
systematic 
overview 
of 
what 
spam 
is 
and 
how 
you 
can 
detect 
it 
as 
such, 
types 
of 
classiers 
and 
ways 
to 
lter 
incoming 
messages 
among 
which 
whitelisting, 
collaborative 
ltering 
and 
Challenge-Response 
systems. 
It 
also 
discusses 
automatic 
learning 
for 
ne-tuning 
the 
lter, 
in 
several 
ways. 


In 
[8] 
several 
machine 
learning 
examples 
are 
summarized, 
and 
each 
is 
explained 
with 
the 
results 
of 
inuential 
papers 
on 
the 
applicable 
subjects. 
For 
instance 
the 
work 
of 
[12] 
is 
presented 
where 
bigrams 
lead 
to 
an 
improved 
performance 
for 
a 
nave 
bayesian 
classier 
compared 
to 
unigrams 
or, 
because 
of 
performance 
tradeos, 
trigrams. 


Other 
surveys 
that 
contain 
the 
methods 
discussed 
in 
this 
paper 
give 
an 
overview 
of 
a 
yearly 
spam-detection 
contest 
and 
the 
properties 
of 
the 
best-
ranking 
approach, 
based 
on 
the 
lter 
submissions 
prior 
to 
a 
TREC 
conference[9– 
11]. 


3 
Evaluated 
Methods 


Two 
of 
the 
commonly 
used 
approaches 
to 
spam 
ltering 
will 
be 
addressed 
in 
this 
section: 
Markov 
models 
and 
N-gram-based 
techniques. 
For 
N-grams 
it 
is 
important 
to 
rst 
explain 
the 
Nave 
Bayesian 
Classier 
in 
more 
detail. 


3.1 
Nave 
Bayes 
Bayes’ 
framework 
for 
conditional 
probabilities 
has 
the 
useful 
quality 
that 
it 
allows 
for 
the 
updating 
of 
a 
probability 
based 
on 
new 
evidence 
from 
multiple 
sources. 
In 
this 
classier, 
a 
given 
unclassied 
e-mail 
message 
is 
denoted 
as 
x 
and 
the 
probability 
that 
a 
given 
message 
x 
belongs 
to 
either 
s 
(spam) 
or 
h 
(ham) 
is 
denoted 
as 
P 
(sjx) 
or 
P 
(hjx). 
Bayes’ 
law 
of 
conditional 
probability 
now 
states 
that 


P 
(xjs)P 
(s)

P 
(sjx) 
= 
(1)

P 
(x) 



Survey: 
spam 
ltering 
techniques 
3 


where 
P 
(x)= 
P 
(xjs)P 
(s)+ 
P 
(xjh)P 
(h). 
Here 
P 
(x) 
is 
the 
probability 
that 
any 
random 
message 
looks 
like 
message 
x 
and 
P 
(xjs) 
is 
the 
probability 
that 
a 
message 
of 
the 
spam 
category 
looks 
like 
x. 


For 
computing 
these 
last 
two 
probabilities 
the 
message 
is 
split 
up 
into 
tokens. 
For 
each 
token 
the 
chance 
of 
occurrance 
in 
spam 
messages 
and 
in 
non-spam 
messages 
is 
recorded 
from 
a 
training 
set 
of 
messages. 
Next 
it 
is 
assumed 
that 
occurrances 
of 
two 
seperate 
tokens 
is 
statistically 
independent, 
and 
the 
probability 
of 
a 
spam-or 
ham-message 
being 
message 
x 
can 
be 
calculated 
by 
multiplying 
all 
the 
chances 
of 
the 
observed 
token 
occurrances. 


Token 
selection 
What 
kind 
of 
tokens 
are 
selected 
is 
the 
main 
dierence 
between 
dierent 
nave 
Bayes 
classiers. 
By 
default 
each 
word 
can 
be 
selected 
as 
a 
token, 
so 
that 
for 
each 
word 
a 
seperate 
chance 
of 
occurring 
in 
spam 
and 
ham 
messages 
is 
calculated. 
This 
means 
that 
information 
about 
the 
retrieval 
of 
lost 
space 
objects 
and 
information 
retrieval 
on 
space 
stations 
would 
rank 
the 
same 
when 
compared 
to 
the 
subject 
of 
information 
retrieval, 
indicating 
that 
topics 
of 
two 
or 
more 
words 
wouldn't 
be 
visible 
in 
this 
representation 
and 
typical 
spam 
subjects 
(free 
money) 
would 
be 
rated 
inaccurately 
because 
of 
legitimate 
uses 
in 
sentences 
or 
messages 
containing 
both 
words 
in 
an 
unrelated 
way. 


3.2 
N-grams 
To 
reintroduce 
this 
interword 
dependency 
N 
adjacent 
words 
(N-grams) 
can 
be 
selected 
as 
a 
token. 
In 
information 
retrieval 
on 
space 
stations 
the 
2grams 
(bigrams) 
information 
retrieval, 
retrieval 
on, 
on 
space, 
space 
stations 
would 
be 
selected. 
Here 
you 
would 
gain 
signicant 
topical 
precision 
when 
matching 
it 
to 
the 
document 
representation 
of 
a 
message 
that 
is 
to 
be 
classied 
as 
spam 
or 
ham, 
but 
in 
the 
process 
you 
would 
lose 
the 
ability 
to 
classify 
regular 
messages 
that 
don't 
share 
a 
topical 
similarity 
to 
a 
message 
from 
the 
training 
corpus. 
This 
can 
be 
solved 
again 
by 
recording 
the 
single 
words 
(unigrams) 
as 
well 
as 
the 
bigrams. 
For 
every 
additional 
set 
of 
n-grams 
that 
is 
recorded, 
a 
more 
detailed 
spam 
classication 
can 
be 
made. 


To 
include 
the 
interaction 
between 
nearby 
words 
in 
sentences 
that 
don't 
succeed 
eachother, 
without 
including 
ever 
longer 
n-grams, 
sparse 
bigrams 
can 
be 
used: 
a 
combination 
of 
two 
words 
that 
occur 
together 
in 
a 
sentence, 
seperated 
by 
no 
more 
than 
n 
words, 
without 
the 
words 
occurring 
inbetween. 
For 
example, 
I 
think 
therefore 
I 
am 
with 
n 
= 
2 
would 
generate 
I 
think, 
I 
therefore, 
I 
I, 
think 
therefore, 
think 
I, 
think 
am, 
therefore 
I, 
therefore 
am, 
I 
am 
as 
sparse 
bigrams. 
The 
greater 
n, 
the 
less 
interaction 
there 
can 
be 
assumed 
to 
be 
among 
the 
nearby 
words. 
To 
make 
use 
of 
the 
distance 
between 
the 
two 
words 
in 
the 
bigram, 
the 
distance 
can 
be 
recorded 
in 
the 
bigram 
by 
putting 
a 
? 
for 
every 
omitted 
word, 
e.g. 
think+?+?+am. 
This 
method 
is 
called 
Orthogonal 
sparse 
bigrams[7] 
(OSB). 



Michiel 
van 
Dam 


Finally 
n-grams 
can 
be 
based 
on 
characters 
as 
well 
as 
words. 
Character 
4-grams 
for 
the 
previous 
example 
would 
be 
I 
th, 
thi, 
thin, 
hink, 
ink 
, 
nk 
t 
and 
so 
on, 
where 
represents 
a 
space. 


3.3 
Markov 
Models 
A 
Markov 
model 
is 
a 
system 
where 
the 
next 
value 
is 
predicted 
based 
on 
the 
preceding 
k 
values. 
For 
text 
messages, 
the 
characters 
are 
used 
as 
values, 
so 
a 
markov 
model 
would 
be 
modeling 
the 
probability 
that 
after 
the 
characters 
to 
thin 
the 
character 
k 
will 
appear 
next 
[2]. 
When 
matching 
a 
new 
message 
to 
the 
model, 
the 
probability 
of 
each 
character 
of 
the 
message 
is 
assessed 
based 
on 
the 
preceding 
characters. 
If 
the 
k 
preceding 
characters 
don't 
appear 
in 
the 
training 
set, 
the 
longest 
preceding 
set 
of 
characters 
that 
appears 
in 
the 
model 
is 
taken 
instead 
for 
determining 
the 
probability 
of 
the 
actual 
character 
that 
follows. 


The 
resulting 
chances 
are 
combined 
as 
independent 
probabilities. 
Because 
the 
chance 
of 
each 
character 
occurance 
on 
each 
spot 
is 
0 
<p< 
1, 
this 
probability 
converges 
to 
zero 
for 
long 
documents, 
making 
a 
normalizing 
factor 
necessary, 
based 
on 
the 
document 
length.[2] 
The 
normalizing 
factor 
does 
not 
inuence 
the 
classication 
outcome, 
but 
rather 
makes 
document 
probabilities 
independent 
of 
document 
length, 
which 
is 
useful 
for 
threshold-based 
decisions. 


A 
classication 
based 
such 
a 
character 
Markov 
model 
can 
also 
be 
adapted 
to 
incorporate 
context 
chains 
of 
dierent 
levels, 
discounted 
by 
the 
amount 
of 
preceding 
characters. 
A 
match 
where 
the 
preceding 
eight 
characters 
are 
the 
same 
should 
be 
valued 
higher 
than 
a 
match 
where 
only 
the 
previous 
two 
characters 
are 
the 
same. 


The 
granularity 
of 
such 
markov 
chains 
can 
be 
made 
arbitrarily 
high, 
from 
whole 
sentences 
as 
tokens 
to 
single 
bits 
in 
the 
bitstream. 
For 
example, 
in 
[1] 
a 
dynamic 
Markov 
compression 
is 
used 
as 
a 
model 
where 
each 
bit 
in 
the 
message 
bitstream 
is 
given 
a 
probability 
based 
on 
the 
preceding 
bits. 


3.4 
Using 
N-grams 
within 
Markov 
models 
For 
building 
a 
Markov 
model, 
tokens 
need 
to 
be 
selected 
for 
building 
the 
prediction 
chains. 
So 
far 
for 
Markov 
models, 
character 
tokens 
were 
assumed 
but 
of-
course 
word 
tokens 
or 
n-word 
tokens 
(n-grams) 
are 
also 
possible. 
Usual 
Markov 
models 
use 
around 
5 
context 
characters 
because 
increasing 
the 
context 
order 
beyond 
that 
usually 
results 
in 
a 
decrease 
of 
performance[14]. 
Because 
of 
this, 
the 
results 
for 
Markov 
model 
based 
techniques 
can 
not 
reliably 
be 
evaluated 
against 
N-gram 
based 
techniques 
where 
Markov 
models 
are 
also 
using 
N-grams. 
For 
evaluation 
the 
approaches 
need 
to 
be 
judged 
on 
merit, 
when 
comparing 
techniques 
using 
Markov 
models 
with 
(all) 
other 
techniques, 
and 
when 
comparing 
techniques 
using 
unigrams 
to 
techniques 
using 
bi-or 
N-grams. 



Survey: 
spam 
ltering 
techniques 


4 
Comparison 


Where 
the 
previous 
section 
gives 
an 
introduction 
to 
both 
approaches, 
this 
section 
focuses 
on 
the 
benets 
and 
downsides 
of 
either 
approach. 
The 
approaches 
can 
be 
judged 
on 
memory 
requirements 
but 
also 
on 
the 
speed 
of 
classication 
(eciency) 
and 
on 
the 
correctness 
of 
classications 
(eectiveness). 


4.1 
Memory 
A 
clear 
downside 
of 
n-grams 
is 
the 
increased 
number 
of 
features 
that 
have 
to 
be 
stored: 
the 
number 
of 
possible 
combinations 
of 
words 
is 
exponential 
with 
regard 
to 
n. 
For 
trigrams 
it 
already 
shows 
that 
the 
additional 
computations 
required 
don't 
weigh 
up 
to 
the 
additional 
detail 
that 
is 
captured, 
because 
topics 
consisting 
of 
three 
words 
(like 
world 
trade 
centre) 
occur 
very 
sparsely 
as 
bigrams 
(e.g. 
world 
trade 
and 
trade 
centre) 
when 
not 
discussing 
the 
same 
topic[7]. 


For 
Markov 
models 
memory 
is 
also 
an 
issue. 
In 
[2] 
it 
was 
found 
that 
the 
model 
needed 
to 
be 
pruned 
after 
training 
on 
roughly 
15.000 
sample 
messages 
due 
to 
memory 
constraints. 
When 
the 
maximum 
number 
of 
pre-recorded 
characters 
is 
6 
at 
most, 
when 
estimating 
around 
100 
possible 
characters, 
including 
punctuation, 
in 
every 
position, 
the 
maximum 
number 
of 
character 
6-grams 
is 
1012. 
Because 
the 
number 
of 
words 
in 
the 
English 
language 
has 
been 
estimated 
to 
be 
more 
than 
106 
as 
of 
2012[13], 
this 
means 
there 
are 
roughly 
as 
much 
character 
6-grams 
as 
possible 
bigrams 
in 
the 
English 
language. 


4.2 
Spam 
detection 
For 
the 
TREC 
2005 
corpora 
the 
PPM[2] 
(Prediction 
by 
Partial 
Matching) 
approach 
using 
a 
Markov 
model 
performed 
clearly 
the 
best 
in 
all 
instances[7], 
leading 
to 
it 
becoming 
a 
standard 
to 
test 
against 
for 
TREC 
in 
next 
years. 
All 
bagof-
words 
approaches 
(including 
nave 
bayesian 
classiers 
using 
n-grams) 
were 
left 
far 
behind. 
The 
PPM 
Markov 
model 
was 
used 
on 
the 
TREC 
2005 
corpora 
using 
either 
4, 
6 
or 
8 
context 
characters, 
and 
it 
was 
shown 
that 
the 
nature 
of 
the 
corpus 
had 
more 
inuence 
on 
PPM 
performance 
than 
the 
number 
of 
context 
characters. 
While 
6 
characters 
performed 
overall 
best 
out 
of 
these 
three 
options, 
the 
model 
seems 
robust 
for 
the 
choice 
of 
this 
parameter[2]. 


For 
TREC 
2006 
the 
general 
lter 
eectiveness 
rose 
and 
several 
other 
approaches 
came 
close 
to 
PPM. 
OSBF-Lua[4], 
which 
is 
based 
on 
orthogonal 
sparse 
bigrams, 
was 
shown 
to 
have 
several 
approaches 
that 
equaled 
or 
even 
bettered 
PPM[7]. 
However, 
OSBF-Lua 
was 
considered 
by 
the 
authors 
to 
perform 
this 
well 
because 
of 
their 
enhanced 
training 
method. 


For 
TREC 
2007 
the 
best 
performing 
model 
was 
ROSVM[5] 
which 
uses 
binary 
features 
and 
character 
4-grams. 
Its 
success 
was 
also 
caused 
by 
its 
online 
active 
learning 
features, 
updating 
the 
model 
when 
new 
messages 
have 
been 
processed. 


For 
learning 
features, 
it 
is 
suspected 
that 
much 
improvement 
can 
still 
be 
made 
for 
the 
PPM 
approach 
that 
appeared 
in 
2005. 
All 
approaches 
that 
performed 
well 
on 
TREC 
2005 
were 
considered 
fairly 
weak 
in 
terms 
of 
machine 
learning[7]. 



Michiel 
van 
Dam 


Because 
the 
n-gram 
based 
approaches 
were 
evolving 
rapidly 
in 
TREC 
2006 
and 
TREC 
2007, 
they 
surpassed 
PPM 
in 
machine 
learning. 
This 
trend 
was 
visible 
for 
all 
compression 
models[11]. 
However, 
the 
PPM 
method 
has 
still 
be 
improved 
by 
using 
Dynamic 
Markov 
Compression 
and 
using 
only 
a 
2500 
byte 
prex 
of 
the 
message 
for 
classication 
rather 
than 
the 
entire 
messace, 
which 
led 
to 
improved 
eciency 
and, 
more 
surprising, 
to 
more 
eectiveness[7]. 


5 
Conclusions 


Spam 
lters 
are 
improving 
rapidly 
and 
so 
is 
the 
best 
approach 
to 
ltering. 
Where 
Markov 
compression 
was 
the 
best 
lter 
in 
2005, 
more 
recently 
it 
has 
been 
surpassed 
by 
newer 
models. 
Meanwhile 
both 
N-gram 
based 
approaches 
and 
Markov 
models 
have 
proven 
that 
they 
can 
increase 
ltering 
eectiveness. 


Both 
for 
N-grams 
and 
Markov 
context 
chains 
there 
is 
a 
tradeoff 
between 
ef
ciency 
and 
eectiveness. 
For 
Markov 
context 
chains 
it 
is 
known 
that 
for 
more 
than 
5 
tokens 
of 
context 
the 
eciency 
degrades 
too 
much 
compared 
to 
the 
eectiveness 
gain. 
For 
N-gram 
approaches 
the 
orthogonal 
sparse 
bigrams 
appeared 
to 
have 
most 
merit 
for 
eective 
spam 
recognition, 
providing 
a 
good 
tradeoff 
between 
eciency 
while 
still 
giving 
a 
good 
eectiveness. 


Based 
on 
more 
recent 
performances 
of 
both 
models, 
it 
can 
be 
concluded 
that 
N-gram 
based 
approaches 
like 
orthogonal 
sparse 
bigrams 
have 
surpassed 
markov 
models 
in 
terms 
of 
both 
eciency 
and 
eectiveness, 
as 
of 
2007. 


References 


1. 
Bratko, 
A., 
Cormack, 
G.V., 
Filipic, 
B., 
Lynam, 
R., 
Zupan, 
B.: 
Spam 
Filtering 
Using 
Statistical 
Data 
Compression 
Models. 
In 
Proceedings 
of 
TREC, 
2005. 
2. 
Bratko, 
A., 
Filipic, 
B.: 
Spam 
Filtering 
using 
Character-level 
Markov 
Models: 
Experiments 
for 
the 
TREC 
2005 
Spam 
Track. 
In 
Proceedings 
of 
TREC, 
2005. 
3. 
Keselj, 
V., 
Milios, 
E., 
Tuttle, 
A., 
Wang, 
S., 
Zhang, 
R.: 
Spam 
Filtering 
using 
Ngram-
based 
Techniques. 
In 
Proceedings 
of 
TREC, 
2005. 
4. 
Assaz, 
F.: 
OSBFLua, 
A 
text 
classication 
module 
for 
Lua, 
The 
importance 
of 
the 
training 
method. 
In 
Proceedings 
of 
TREC, 
2006. 
5. 
Sculley, 
D., 
Wachman, 
G.M.: 
Relaxed 
online 
SVMs 
in 
the 
TREC 
Spam 
ltering 
track. 
In 
Sixteenth 
Text 
REtrieval 
Conference, 
2007. 
6. 
The 
CRM114 
Team: 
Seven 
Hypothesis 
about 
Spam 
Filtering. 
In 
Proceedings 
of 
TREC, 
2006. 
7. 
Cormack, 
G.V.: 
Email 
Spam 
Filtering: 
A 
Systematic 
Review. 
In 
Foundations 
and 
Trends 
in 
Information 
Retrieval 
archive, 
Volume 
1 
Issue 
4, 
April 
2007. 
8. 
Guzella, 
T.S., 
Caminhas, 
W.M.: 
A 
review 
of 
machine 
learning 
approaches 
to 
Spam 
Filtering. 
Expert 
Systems 
with 
Applications, 
Elsevier, 
Volume 
36, 
Issue 
7, 
Septembe
r 
2009, 
Pages 
1020610222 
9. 
Cormack, 
G.V., 
Lynam, 
T., 
TREC 
2005 
Spam 
Track 
Overview. 
In 
Proceedings 
of 
TREC, 
2005. 
10. 
Cormack, 
G.V.: 
TREC 
2006 
Spam 
Track 
Overview. 
In 
Proceedings 
of 
TREC, 
2006. 
11. 
Cormack, 
G.V.: 
TREC 
2007 
Spam 
Track 
Overview. 
In 
Proceedings 
of 
TREC, 
2007. 

Survey: 
spam 
ltering 
techniques 


12. 
Medlock, 
B.: 
An 
adaptive, 
semi-structured 
language 
model 
approach 
to 
spam 
ltering 
on 
a 
new 
corpus. 
In 
Proceedings 
of 
the 
third 
conference 
on 
email 
and 
anti-spam. 
13. 
Global 
Language 
Monitor: 
Number 
of 
Words 
in 
the 
English 
Language: 
1,010,649.7 
http://www.languagemonitor.com/global-english/ 
number-of-words-in-the-english-language-1008879/ 
14. 
Teahan, 
W. 
J.: 
Text 
classication 
and 
segmentation 
using 
minimum 
cross-entropy. 
In 
Proceedings 
of 
RIAO-00, 
6th 
International 
Conference 
Recherche 
dInformation 
Assistee 
par 
Ordinateur, 
2000 

